"""
LLM pipeline for the hybridRAG system.
"""

from typing import List, Dict, Any, Optional
from config.settings import get_settings
from retrieval.hybrid_retriever import HybridRetriever
from embeddings.generator import EmbeddingGenerator

class LLMPipeline:
    """LLM pipeline that orchestrates retrieval and generation."""
    
    def __init__(self, uri: str = None, user: str = None, password: str = None):
        """Initialize the LLM pipeline."""
        settings = get_settings()
        self.uri = uri or settings.neo4j_uri
        self.user = user or settings.neo4j_user
        self.password = password or settings.neo4j_password
        
        self.retriever = HybridRetriever(self.uri, self.user, self.password)
        self.embedding_generator = EmbeddingGenerator()
        
        # Pipeline configuration
        self.max_context_length = 4000
        self.temperature = 0.7
        self.max_tokens = 500
    
    def close(self):
        """Close all connections."""
        self.retriever.close()
    
    def process_query(self, query: str, strategy: str = "hybrid", 
                     top_k: int = 5, include_explanation: bool = False) -> Dict[str, Any]:
        """Process a query through the complete pipeline."""
        try:
            # Step 1: Retrieve relevant documents
            retrieved_docs = self.retriever.retrieve(query, top_k, strategy)
            
            # Step 2: Prepare context from retrieved documents
            context = self._prepare_context(retrieved_docs)
            
            # Step 3: Generate response (placeholder for actual LLM call)
            response = self._generate_response(query, context)
            
            # Step 4: Prepare result
            result = {
                'query': query,
                'strategy': strategy,
                'retrieved_documents': len(retrieved_docs),
                'response': response,
                'context_length': len(context)
            }
            
            if include_explanation:
                result['explanation'] = self.retriever.explain_retrieval(query, top_k)
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'query': query,
                'strategy': strategy
            }
    
    def _prepare_context(self, retrieved_docs: List[tuple]) -> str:
        """Prepare context string from retrieved documents."""
        context_parts = []
        
        for i, (node, score) in enumerate(retrieved_docs):
            # Extract text content from the node
            text = getattr(node, 'text', str(node))
            context_parts.append(f"Document {i+1} (Score: {score:.3f}):\n{text}\n")
        
        context = "\n".join(context_parts)
        
        # Truncate if too long
        if len(context) > self.max_context_length:
            context = context[:self.max_context_length] + "..."
        
        return context
    
    def _generate_response(self, query: str, context: str) -> str:
        """Generate response using the context (placeholder implementation)."""
        # This is a placeholder - in a real implementation, you would:
        # 1. Call an actual LLM API (OpenAI, Anthropic, etc.)
        # 2. Use a prompt template
        # 3. Handle the response generation
        
        prompt = f"""
        Based on the following context, please answer the question.
        
        Question: {query}
        
        Context:
        {context}
        
        Answer:
        """
        
        # For now, return a simple response
        # In production, replace this with actual LLM call
        return f"Based on the retrieved documents, here's what I found about '{query}': [This is a placeholder response. In production, this would be generated by an actual LLM using the context above.]"
    
    def batch_process(self, queries: List[str], strategy: str = "hybrid", 
                     top_k: int = 5) -> List[Dict[str, Any]]:
        """Process multiple queries in batch."""
        results = []
        
        for query in queries:
            result = self.process_query(query, strategy, top_k)
            results.append(result)
        
        return results
    
    def set_pipeline_config(self, max_context_length: int = None, 
                           temperature: float = None, max_tokens: int = None):
        """Set pipeline configuration parameters."""
        if max_context_length is not None:
            self.max_context_length = max_context_length
        if temperature is not None:
            self.temperature = temperature
        if max_tokens is not None:
            self.max_tokens = max_tokens
    
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """Get pipeline statistics and configuration."""
        return {
            'max_context_length': self.max_context_length,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'retriever_weights': {
                'vector': self.retriever.vector_weight,
                'fulltext': self.retriever.fulltext_weight,
                'semantic': self.retriever.semantic_weight
            }
        }
